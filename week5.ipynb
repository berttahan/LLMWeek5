{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:16.140060Z","iopub.execute_input":"2025-02-16T17:28:16.140246Z","iopub.status.idle":"2025-02-16T17:28:17.049516Z","shell.execute_reply.started":"2025-02-16T17:28:16.140227Z","shell.execute_reply":"2025-02-16T17:28:17.048653Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade category-encoders gensim mlxtend plotnine","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: category-encoders in /usr/local/lib/python3.10/dist-packages (2.8.0)\nRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\nRequirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.23.4)\nRequirement already satisfied: plotnine in /usr/local/lib/python3.10/dist-packages (0.14.5)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (1.26.4)\nRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (2.2.2)\nRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (1.0.1)\nRequirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (1.6.1)\nRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (1.13.0)\nRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders) (0.14.4)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\nRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.9.0)\nRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.4.2)\nRequirement already satisfied: mizani~=0.13.0 in /usr/local/lib/python3.10/dist-packages (from plotnine) (0.13.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->category-encoders) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category-encoders) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category-encoders) (2024.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.6.0->category-encoders) (3.5.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->category-encoders) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->category-encoders) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->category-encoders) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->category-encoders) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->category-encoders) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install matplotlib==3.9.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:30:20.874425Z","iopub.execute_input":"2025-02-16T17:30:20.874782Z","iopub.status.idle":"2025-02-16T17:30:27.289804Z","shell.execute_reply.started":"2025-02-16T17:30:20.874753Z","shell.execute_reply":"2025-02-16T17:30:27.288981Z"}},"outputs":[{"name":"stdout","text":"Collecting matplotlib==3.9.0\n  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (1.4.7)\nRequirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (24.2)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.0) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23->matplotlib==3.9.0) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23->matplotlib==3.9.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23->matplotlib==3.9.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.23->matplotlib==3.9.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.23->matplotlib==3.9.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.23->matplotlib==3.9.0) (2024.2.0)\nDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.10.0\n    Uninstalling matplotlib-3.10.0:\n      Successfully uninstalled matplotlib-3.10.0\nSuccessfully installed matplotlib-3.9.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install scipy==1.13.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:30:47.358485Z","iopub.execute_input":"2025-02-16T17:30:47.358868Z","iopub.status.idle":"2025-02-16T17:30:50.719564Z","shell.execute_reply.started":"2025-02-16T17:30:47.358814Z","shell.execute_reply":"2025-02-16T17:30:50.718607Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy==1.13.0 in /usr/local/lib/python3.10/dist-packages (1.13.0)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy==1.13.0) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy==1.13.0) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy==1.13.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy==1.13.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy==1.13.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy==1.13.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy==1.13.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install -U bitsandbytes transformers peft accelerate datasets einops evaluate trl rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:30:57.340749Z","iopub.execute_input":"2025-02-16T17:30:57.341102Z","iopub.status.idle":"2025-02-16T17:31:14.485956Z","shell.execute_reply.started":"2025-02-16T17:30:57.341074Z","shell.execute_reply":"2025-02-16T17:31:14.485087Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting accelerate\n  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting datasets\n  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nCollecting einops\n  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting trl\n  Downloading trl-0.15.0-py3-none-any.whl.metadata (11 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-3.3.0-py3-none-any.whl (484 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.0-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b9f6399429575ff7b13d698c21e7bbe0fa7da546809eb434d0b028d5706adef7\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: einops, transformers, datasets, accelerate, trl, rouge-score, evaluate, bitsandbytes\n  Attempting uninstall: einops\n    Found existing installation: einops 0.8.0\n    Uninstalling einops-0.8.0:\n      Successfully uninstalled einops-0.8.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.2.0\n    Uninstalling datasets-3.2.0:\n      Successfully uninstalled datasets-3.2.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.3.0 bitsandbytes-0.45.2 datasets-3.3.0 einops-0.8.1 evaluate-0.4.3 rouge-score-0.1.2 transformers-4.48.3 trl-0.15.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Fixed an error, added versions to packages: !pip install -U scipy==1.13.0 scikit-learn==1.3.1 matplotlib==3.8.0","metadata":{}},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nimport importlib.metadata\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\nprint(\"Rouge Score version:\", importlib.metadata.version(\"rouge-score\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:31:29.397075Z","iopub.execute_input":"2025-02-16T17:31:29.397391Z","iopub.status.idle":"2025-02-16T17:31:49.073933Z","shell.execute_reply.started":"2025-02-16T17:31:29.397364Z","shell.execute_reply":"2025-02-16T17:31:49.073048Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.48.3\nPEFT version: 0.14.0\nAccelerate version: 1.3.0\nDatasets version: 3.3.0\nScipy version: 1.13.0\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.0\nRouge Score version: 0.1.2\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Fixed error, added: import importlib.metadata\nprint(\"Rouge Score version:\", importlib.metadata.version(\"rouge_score\")) (rouge_score library does not have __version__ attribute.)","metadata":{}},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"\nprint(\"OK\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:32:07.577536Z","iopub.execute_input":"2025-02-16T17:32:07.578233Z","iopub.status.idle":"2025-02-16T17:32:07.583367Z","shell.execute_reply.started":"2025-02-16T17:32:07.578197Z","shell.execute_reply":"2025-02-16T17:32:07.582354Z"}},"outputs":[{"name":"stdout","text":"OK\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:32:16.205667Z","iopub.execute_input":"2025-02-16T17:32:16.206012Z","iopub.status.idle":"2025-02-16T17:32:30.826071Z","shell.execute_reply.started":"2025-02-16T17:32:16.205985Z","shell.execute_reply":"2025-02-16T17:32:30.825220Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  Y\n"},{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:33:06.153980Z","iopub.execute_input":"2025-02-16T17:33:06.154298Z","iopub.status.idle":"2025-02-16T17:33:06.158735Z","shell.execute_reply.started":"2025-02-16T17:33:06.154263Z","shell.execute_reply":"2025-02-16T17:33:06.157923Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:33:16.920564Z","iopub.execute_input":"2025-02-16T17:33:16.920894Z","iopub.status.idle":"2025-02-16T17:33:19.561976Z","shell.execute_reply.started":"2025-02-16T17:33:16.920869Z","shell.execute_reply":"2025-02-16T17:33:19.560980Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3569ec162a024413a2b98b94a40e5e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8efa9c342ce7432cbc0e059e99e6286f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6a0880b5bd49068d09ce26ea5935f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd215f566d645cd9f1f75ddadf84b89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52f6c3535484b71bf8ae2ba9045164b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fb4b368161487d80888e8803b2b93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55264f94ef44cb6a3af45770b2de968"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:35:30.909599Z","iopub.execute_input":"2025-02-16T17:35:30.909975Z","iopub.status.idle":"2025-02-16T17:35:30.917363Z","shell.execute_reply.started":"2025-02-16T17:35:30.909947Z","shell.execute_reply":"2025-02-16T17:35:30.916593Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:35:43.189714Z","iopub.execute_input":"2025-02-16T17:35:43.190075Z","iopub.status.idle":"2025-02-16T17:35:43.195964Z","shell.execute_reply.started":"2025-02-16T17:35:43.190043Z","shell.execute_reply":"2025-02-16T17:35:43.195127Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:35:49.859640Z","iopub.execute_input":"2025-02-16T17:35:49.860010Z","iopub.status.idle":"2025-02-16T17:36:45.444757Z","shell.execute_reply.started":"2025-02-16T17:35:49.859982Z","shell.execute_reply":"2025-02-16T17:36:45.444156Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d778a56132f4cedabd8791ecfd12eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72ce08284a8463b9b461db21e6569cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cb5955173e40029f2020f1c33201ec"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"fixed an error: use_auth_token changed to token.","metadata":{}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:13.988624Z","iopub.execute_input":"2025-02-16T17:37:13.989006Z","iopub.status.idle":"2025-02-16T17:37:15.304495Z","shell.execute_reply.started":"2025-02-16T17:37:13.988974Z","shell.execute_reply":"2025-02-16T17:37:15.303588Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e4e1709c654bd3a20d92f08f070292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1a8fe3d4f32411a8f136f8b3f098bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd83bee59b24c11977efbf4f5a835a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9fc58c712c74a5389652b56e1b4e982"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:19.630987Z","iopub.execute_input":"2025-02-16T17:37:19.631321Z","iopub.status.idle":"2025-02-16T17:37:19.636096Z","shell.execute_reply.started":"2025-02-16T17:37:19.631283Z","shell.execute_reply":"2025-02-16T17:37:19.635310Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1224 MB.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:26.489955Z","iopub.execute_input":"2025-02-16T17:37:26.490259Z","iopub.status.idle":"2025-02-16T17:37:26.730711Z","shell.execute_reply.started":"2025-02-16T17:37:26.490236Z","shell.execute_reply":"2025-02-16T17:37:26.729944Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:30.134143Z","iopub.execute_input":"2025-02-16T17:37:30.134456Z","iopub.status.idle":"2025-02-16T17:37:33.747988Z","shell.execute_reply.started":"2025-02-16T17:37:30.134432Z","shell.execute_reply":"2025-02-16T17:37:33.747102Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nBrian: Happy Birthday, this is for you, Brian.\nPerson2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\nPerson1: Brian, may I have a pleasure to have a dance with you?\nPerson2: Ok.\nPerson1: This is really wonderful party.\nPerson2: Yes, you are always popular with everyone. and you\nCPU times: user 3.1 s, sys: 52.2 ms, total: 3.15 s\nWall time: 3.61 s\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:41.788338Z","iopub.execute_input":"2025-02-16T17:37:41.788643Z","iopub.status.idle":"2025-02-16T17:37:41.793423Z","shell.execute_reply.started":"2025-02-16T17:37:41.788621Z","shell.execute_reply":"2025-02-16T17:37:41.792656Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:50.118356Z","iopub.execute_input":"2025-02-16T17:37:50.118661Z","iopub.status.idle":"2025-02-16T17:37:50.123803Z","shell.execute_reply.started":"2025-02-16T17:37:50.118637Z","shell.execute_reply":"2025-02-16T17:37:50.122951Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:37:59.760286Z","iopub.execute_input":"2025-02-16T17:37:59.760600Z","iopub.status.idle":"2025-02-16T17:37:59.765625Z","shell.execute_reply.started":"2025-02-16T17:37:59.760574Z","shell.execute_reply":"2025-02-16T17:37:59.764615Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:04.213294Z","iopub.execute_input":"2025-02-16T17:38:04.213608Z","iopub.status.idle":"2025-02-16T17:38:04.217819Z","shell.execute_reply.started":"2025-02-16T17:38:04.213584Z","shell.execute_reply":"2025-02-16T17:38:04.216819Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1286 MB.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:07.980159Z","iopub.execute_input":"2025-02-16T17:38:07.980522Z","iopub.status.idle":"2025-02-16T17:38:13.959130Z","shell.execute_reply.started":"2025-02-16T17:38:07.980483Z","shell.execute_reply":"2025-02-16T17:38:13.958264Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629bdcce2ea8436e924ec39701b495fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980ccc9cb902486f94e61b4e4ac89656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ace2b0bdc44d489e0436569a8d7ae4"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22c87b125f664a739ac688c46305d276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71775903ce3c4672b8c07f6781175f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5008d40f71424d758f21545acc6115a4"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:30.280341Z","iopub.execute_input":"2025-02-16T17:38:30.280660Z","iopub.status.idle":"2025-02-16T17:38:30.287201Z","shell.execute_reply.started":"2025-02-16T17:38:30.280639Z","shell.execute_reply":"2025-02-16T17:38:30.286416Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 131164160\nall model parameters: 615606272\npercentage of trainable model parameters: 21.31%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:35.903558Z","iopub.execute_input":"2025-02-16T17:38:35.903884Z","iopub.status.idle":"2025-02-16T17:38:35.909311Z","shell.execute_reply.started":"2025-02-16T17:38:35.903858Z","shell.execute_reply":"2025-02-16T17:38:35.908094Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:42.034893Z","iopub.execute_input":"2025-02-16T17:38:42.035215Z","iopub.status.idle":"2025-02-16T17:38:42.191940Z","shell.execute_reply.started":"2025-02-16T17:38:42.035190Z","shell.execute_reply":"2025-02-16T17:38:42.191237Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:46.907318Z","iopub.execute_input":"2025-02-16T17:38:46.907615Z","iopub.status.idle":"2025-02-16T17:38:46.914668Z","shell.execute_reply.started":"2025-02-16T17:38:46.907592Z","shell.execute_reply":"2025-02-16T17:38:46.913737Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 6127616\nall model parameters: 621733888\npercentage of trainable model parameters: 0.99%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,  # ✅ Increased for better GPU usage\n    gradient_accumulation_steps=2,  # ✅ Reduced for speed\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,  # ✅ Logs less often\n    save_strategy=\"steps\",\n    save_steps=100,  # ✅ Saves less often\n    eval_strategy=\"steps\",\n    eval_steps=50,  # ✅ Evaluates less often\n    do_eval=True,\n    gradient_checkpointing=False,  # ✅ Disabled for speed\n    fp16=True,  # ✅ Enables mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:52.662361Z","iopub.execute_input":"2025-02-16T17:38:52.662700Z","iopub.status.idle":"2025-02-16T17:38:52.699855Z","shell.execute_reply.started":"2025-02-16T17:38:52.662670Z","shell.execute_reply":"2025-02-16T17:38:52.699181Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_training_args.device\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:59.965000Z","iopub.execute_input":"2025-02-16T17:38:59.965442Z","iopub.status.idle":"2025-02-16T18:13:01.541015Z","shell.execute_reply.started":"2025-02-16T17:38:59.965408Z","shell.execute_reply":"2025-02-16T18:13:01.540272Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 33:49, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.433900</td>\n      <td>1.332374</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.256900</td>\n      <td>1.312465</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.288000</td>\n      <td>1.283109</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.267900</td>\n      <td>1.273435</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.275600</td>\n      <td>1.272370</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.237800</td>\n      <td>1.266189</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.250600</td>\n      <td>1.262266</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.233700</td>\n      <td>1.260827</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.224300</td>\n      <td>1.260158</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.232500</td>\n      <td>1.259174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2700997161865235, metrics={'train_runtime': 2040.5965, 'train_samples_per_second': 1.96, 'train_steps_per_second': 0.245, 'total_flos': 7998465185280000.0, 'train_loss': 1.2700997161865235, 'epoch': 2.0})"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"berttahan/TinyLlama-1.1B-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"berttahan/TinyLlama-1.1B-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:16:14.372504Z","iopub.execute_input":"2025-02-16T18:16:14.372893Z","iopub.status.idle":"2025-02-16T18:16:21.064584Z","shell.execute_reply.started":"2025-02-16T18:16:14.372866Z","shell.execute_reply":"2025-02-16T18:16:21.063871Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d00711d443e48029d60ce1589a1816e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87e6f40b41b483b8bdedfcdd339cd3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a57c9ea722c45f2b6953323d95d572d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93544fa5fdff4faea796a27f9fb535a3"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/berttahan/TinyLlama-1.1B-dialogsum-finetuned/commit/812ecabe9d00c2059eccdace5b7660ded590af83', commit_message='Upload tokenizer', commit_description='', oid='812ecabe9d00c2059eccdace5b7660ded590af83', pr_url=None, repo_url=RepoUrl('https://huggingface.co/berttahan/TinyLlama-1.1B-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='berttahan/TinyLlama-1.1B-dialogsum-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"print_gpu_utilization()\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()\nprint_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:16:45.013544Z","iopub.execute_input":"2025-02-16T18:16:45.013921Z","iopub.status.idle":"2025-02-16T18:16:45.141720Z","shell.execute_reply.started":"2025-02-16T18:16:45.013889Z","shell.execute_reply":"2025-02-16T18:16:45.140943Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 8704 MB.\nGPU memory occupied: 1610 MB.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:17:42.691300Z","iopub.execute_input":"2025-02-16T18:17:42.691597Z","iopub.status.idle":"2025-02-16T18:17:55.251838Z","shell.execute_reply.started":"2025-02-16T18:17:42.691576Z","shell.execute_reply":"2025-02-16T18:17:55.250931Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"use_token_auth changed to token, error fixed","metadata":{}},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:18:14.912750Z","iopub.execute_input":"2025-02-16T18:18:14.913109Z","iopub.status.idle":"2025-02-16T18:18:15.201112Z","shell.execute_reply.started":"2025-02-16T18:18:14.913080Z","shell.execute_reply":"2025-02-16T18:18:15.200376Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:19:12.421710Z","iopub.execute_input":"2025-02-16T18:19:12.422092Z","iopub.status.idle":"2025-02-16T18:19:12.610976Z","shell.execute_reply.started":"2025-02-16T18:19:12.422063Z","shell.execute_reply":"2025-02-16T18:19:12.610271Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:19:26.739232Z","iopub.execute_input":"2025-02-16T18:19:26.739582Z","iopub.status.idle":"2025-02-16T18:19:31.106520Z","shell.execute_reply.started":"2025-02-16T18:19:26.739555Z","shell.execute_reply":"2025-02-16T18:19:31.105830Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# invites Brian to the party and thanks him for the birthday party.\n\n###### Step 4: End the conversation\n\n#Person1#: Thanks for the party, Brian. I hope you have a good time.\n#Person2#: Thanks, I'm glad you enjoyed it.\n#Person1#: Bye.\n#Person2#: Bye.\n\n###### End the conversation.\n\n\nCPU times: user 4.35 s, sys: 2.23 ms, total: 4.35 s\nWall time: 4.36 s\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:19:48.961785Z","iopub.execute_input":"2025-02-16T18:19:48.962146Z","iopub.status.idle":"2025-02-16T18:19:51.590123Z","shell.execute_reply.started":"2025-02-16T18:19:48.962119Z","shell.execute_reply":"2025-02-16T18:19:51.589343Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:20:03.916581Z","iopub.execute_input":"2025-02-16T18:20:03.916948Z","iopub.status.idle":"2025-02-16T18:21:06.483720Z","shell.execute_reply.started":"2025-02-16T18:20:03.916918Z","shell.execute_reply":"2025-02-16T18:21:06.482813Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  #Person1#: Ms. Dawson, I need you to take a di...   \n1  #Person1#: Ms. Dawson, I need you to take a di...   \n2  #Person1#: Ms. Dawson, I need you to take a di...   \n3  - The conversation between two people discussi...   \n4  - The conversation between two people discussi...   \n5  - The conversation between two people discussi...   \n6  - The conversation between two people about a ...   \n7  - The conversation between two friends, one of...   \n8  - The conversation between two people about a ...   \n9  Brian: Happy Birthday, this is for you, Brian....   \n\n                                peft_model_summaries  \n0  #Person1# dictates an intra-office memorandum ...  \n1  #Person1# dictates an intra-office memorandum ...  \n2  #Person1# dictates an intra-office memorandum ...  \n3  #Person1# and #Person2# are discussing the adv...  \n4  #Person1# and #Person2# have a conversation ab...  \n5  #Person1# and #Person2# have a conversation ab...  \n6  #Person1# and #Person2# are discussing the div...  \n7  #Person1# and #Person2# are discussing the div...  \n8  #Person1# and #Person2# are discussing the div...  \n9  #Person1# invites Brian to the party and thank...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n      <td>#Person1# dictates an intra-office memorandum ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n      <td>#Person1# dictates an intra-office memorandum ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n      <td>#Person1# dictates an intra-office memorandum ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>- The conversation between two people discussi...</td>\n      <td>#Person1# and #Person2# are discussing the adv...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>- The conversation between two people discussi...</td>\n      <td>#Person1# and #Person2# have a conversation ab...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>- The conversation between two people discussi...</td>\n      <td>#Person1# and #Person2# have a conversation ab...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>- The conversation between two people about a ...</td>\n      <td>#Person1# and #Person2# are discussing the div...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>- The conversation between two friends, one of...</td>\n      <td>#Person1# and #Person2# are discussing the div...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>- The conversation between two people about a ...</td>\n      <td>#Person1# and #Person2# are discussing the div...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Brian: Happy Birthday, this is for you, Brian....</td>\n      <td>#Person1# invites Brian to the party and thank...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:21:26.794987Z","iopub.execute_input":"2025-02-16T18:21:26.795290Z","iopub.status.idle":"2025-02-16T18:21:30.320437Z","shell.execute_reply.started":"2025-02-16T18:21:26.795265Z","shell.execute_reply":"2025-02-16T18:21:30.319367Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:21:38.250405Z","iopub.execute_input":"2025-02-16T18:21:38.250740Z","iopub.status.idle":"2025-02-16T18:21:39.574116Z","shell.execute_reply.started":"2025-02-16T18:21:38.250711Z","shell.execute_reply":"2025-02-16T18:21:39.573380Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a441619442c7412a94c40e38c4e4ec04"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.27205645951314783, 'rouge2': 0.06953689738003464, 'rougeL': 0.17972832555769502, 'rougeLsum': 0.1852886370757599}\nPEFT MODEL:\n{'rouge1': 0.28103226920479896, 'rouge2': 0.07016632746248885, 'rougeL': 0.22293582303576204, 'rougeLsum': 0.2049031934195003}\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:21:49.495552Z","iopub.execute_input":"2025-02-16T18:21:49.495878Z","iopub.status.idle":"2025-02-16T18:21:49.501657Z","shell.execute_reply.started":"2025-02-16T18:21:49.495832Z","shell.execute_reply":"2025-02-16T18:21:49.500903Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 0.90%\nrouge2: 0.06%\nrougeL: 4.32%\nrougeLsum: 1.96%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"I fixed errors along the way like library versions etc. I used model TinyLlama-1.1B in the training. You can find my fine-tuned model from Hugging Face at: https://huggingface.co/berttahan/TinyLlama-1.1B-dialogsum-finetuned\nMy source from GitHub: https://github.com/berttahan/LLMWeek5","metadata":{}}]}